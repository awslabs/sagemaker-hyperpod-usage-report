AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template for AWS Cluster Usage Reporting Infrastructure'

Parameters:
  EKSClusterName:
    Type: String
    Description: Name of the EKS cluster
  HyperPodClusterId:
    Type: String
    Description: Id of the HyperPod cluster
  UsageReportInstallerRoleName:
    Type: String
    Description: Name of the IAM role for usage reporting installation
  DataRententionDays:
    Type: Number
    Description: Data rentention day for S3 Bucket
    Default: 180
  InstallPodIdentityAddon:
    Type: String
    Default: "true"
    AllowedValues: [ "true", "false" ]
    Description: Whether to install the Pod Identity Addon
  UsageReportOperatorNameSpace:
    Type: String
    Description: Kubernetes cluster namespace where usage report operator is installed 
    # The default namespace is the same as the operator namespace specified in helm chart 
    # https://github.com/awslabs/sagemaker-hyperpod-usage-report/blob/main/helm_chart/SageMakerHyperPodUsageReportChart/templates/_helpers.tpl
    Default: "hyperpod-usage-report"
  OperatorServiceAccount:
    Type: String
    Description: Service account used by usage report operator pod identity for permissions to access AWS resources
    # The default service is the same as the one specified in helm chart
    # https://github.com/awslabs/sagemaker-hyperpod-usage-report/blob/main/helm_chart/SageMakerHyperPodUsageReportChart/values.yaml
    Default: "sagemaker-hyperpod-usage-report-service-account"

Conditions:
  ShouldInstallAddon: !Equals
    - !Ref InstallPodIdentityAddon
    - "true"

Resources:
  # S3 Bucket
  UsageReportBucket:
    Type: AWS::S3::Bucket
    Description: "Bucket for cluster usage report data storage."
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      BucketName: !Sub 
      - "${AWS::AccountId}-${AWS::Region}-${HyperPodClusterId}-usage-report-${Suffix}"
      - Suffix: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId]]]]
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: ArchiveOldData
            Status: Enabled
            ExpirationInDays: !Ref DataRententionDays
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
  EksAccessEntry:
    Type: AWS::EKS::AccessEntry
    Properties:
      ClusterName: !Ref EKSClusterName
      PrincipalArn: !Sub 'arn:aws:iam::${AWS::AccountId}:role/${UsageReportInstallerRoleName}'
      Type: STANDARD
      KubernetesGroups: 
        - !Sub ${UsageReportInstallerRoleName}-Role
      Username: !Sub ${UsageReportInstallerRoleName}-Role
  # Pod identity resources setup
  ServiceAccountRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowEksAuthToAssumeRoleForPodIdentity
            Effect: Allow
            Principal:
              Service: pods.eks.amazonaws.com
            Action:
              - 'sts:AssumeRole'
              - 'sts:TagSession'
      Policies:
        - PolicyName: ServiceAccountPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                Resource: 
                  - !Sub 'arn:aws:s3:::${UsageReportBucket}'
                  - !Sub 'arn:aws:s3:::${UsageReportBucket}/*'
  PodIdentityAgentAddon:
    Type: 'AWS::EKS::Addon'
    Condition: ShouldInstallAddon
    Properties:
      ClusterName: !Ref EKSClusterName
      AddonName: 'eks-pod-identity-agent'
  PodIdentityAssociation:
    Type: 'AWS::EKS::PodIdentityAssociation' 
    Properties:
      ClusterName: !Ref EKSClusterName
      Namespace: !Ref UsageReportOperatorNameSpace 
      ServiceAccount: !Ref OperatorServiceAccount 
      RoleArn: !GetAtt ServiceAccountRole.Arn

  # Glue Database
  UsageReportDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: 'Database for AWS Cluster usage report.'

  UsageReportAthenaWorkGroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: !Sub 
      - "${AWS::AccountId}-${AWS::Region}-${HyperPodClusterId}-usage-report-${Suffix}"
      - Suffix: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId]]]]
      WorkGroupConfiguration:
        ResultConfiguration:
          OutputLocation: !Sub "s3://${UsageReportBucket}/athena-results"
      RecursiveDeleteOption: true

  # ClusterQueue Table
  ClusterQueueTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref UsageReportDatabase
      TableInput:
        Name: 'clusterqueue'
        Description: 'Cluster queue metrics data'
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "parquet",
          "parquet.compression": "SNAPPY"
        }
        StorageDescriptor:
          Location: !Sub 's3://${UsageReportBucket}/raw/clusterqueue/'
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Columns:
            - Name: instance_type
              Type: string
            - Name: timestamp
              Type: timestamp
            - Name: namespace
              Type: string
            - Name: resource_version
              Type: string
            - Name: team
              Type: string
            - Name: gpu_total
              Type: double
            - Name: gpu_borrowed
              Type: double
            - Name: cpu_total
              Type: double
            - Name: cpu_borrowed
              Type: double
            - Name: neuron_core_total
              Type: double
            - Name: neuron_core_borrowed
              Type: double
        PartitionKeys:
            - Name: year
              Type: string
            - Name: month
              Type: string
            - Name: day
              Type: string
            - Name: hour
              Type: string
            - Name: cluster
              Type: string

  # Workload Table
  WorkloadTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref UsageReportDatabase
      TableInput:
        Name: 'workload'
        Description: 'Workload data'
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "parquet",
          "parquet.compression": "SNAPPY"
        }
        StorageDescriptor:
          Location: !Sub 's3://${UsageReportBucket}/raw/workload/'
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Columns:
            - Name: workload_name
              Type: string
            - Name: timestamp
              Type: timestamp
            - Name: namespace
              Type: string
            - Name: resource_version
              Type: string
            - Name: team
              Type: string
            - Name: status
              Type: string
            - Name: task_priority_class
              Type: string
            - Name: instance_type
              Type: string
            - Name: CPU
              Type: double
            - Name: GPU
              Type: double
            - Name: neuron_core
              Type: double
            - Name: admitted
              Type: boolean
            - Name: finished
              Type: boolean
        PartitionKeys:
            - Name: year
              Type: string
            - Name: month
              Type: string
            - Name: day
              Type: string
            - Name: hour
              Type: string
            - Name: cluster
              Type: string

  # Pod Table
  PodTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref UsageReportDatabase
      TableInput:
        Name: 'pod'
        Description: 'Pod data'
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "parquet",
          "parquet.compression": "SNAPPY"
        }
        StorageDescriptor:
          Location: !Sub 's3://${UsageReportBucket}/raw/pod/'
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Columns:
            - Name: pod
              Type: string
            - Name: timestamp
              Type: timestamp
            - Name: namespace
              Type: string
            - Name: resource_version
              Type: string
            - Name: status
              Type: string
            - Name: owner
              Type: string
            - Name: instance_type
              Type: string
            - Name: job_type
              Type: string
            - Name: assigned_node
              Type: string
            - Name: CPU
              Type: double
            - Name: GPU
              Type: double
            - Name: neuron_core
              Type: double
            - Name: priority_class
              Type: string
        PartitionKeys:
            - Name: year
              Type: string
            - Name: month
              Type: string
            - Name: day
              Type: string
            - Name: hour
              Type: string
            - Name: cluster
              Type: string
            
  # Detailed Report Table
  DetailedReportTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref UsageReportDatabase
      TableInput:
        Name: 'detailed_report'
        Description: 'Detailed utilization report'
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "parquet",
          "parquet.compression": "SNAPPY"
        }
        StorageDescriptor:
          Location: !Sub 's3://${UsageReportBucket}/reports/detailed/'
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Columns:
            - Name: report_date
              Type: date
            - Name: period_start
              Type: timestamp
            - Name: period_end
              Type: timestamp
            - Name: namespace
              Type: string
            - Name: team
              Type: string
            - Name: task_name
              Type: string
            - Name: instance
              Type: string
            - Name: instance_count
              Type: int
            - Name: status
              Type: string
            - Name: utilized_neuron_core_hours
              Type: double
            - Name: utilized_neuron_core_count
              Type: double
            - Name: utilized_gpu_hours
              Type: double
            - Name: utilized_gpu_count
              Type: double
            - Name: utilized_vcpu_hours
              Type: double
            - Name: utilized_vcpu_count
              Type: double
            - Name: priority_class
              Type: string
        PartitionKeys:
            - Name: year
              Type: string
            - Name: month
              Type: string
            - Name: day
              Type: string
            - Name: cluster
              Type: string

  # Summary Report Table
  SummaryReportTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref UsageReportDatabase
      TableInput:
        Name: 'summary_report'
        Description: 'Summary utilization report'
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "parquet",
          "parquet.compression": "SNAPPY"
        }
        StorageDescriptor:
          Location: !Sub 's3://${UsageReportBucket}/reports/summary/'
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Columns:
            - Name: report_date
              Type: date
            - Name: namespace
              Type: string
            - Name: team
              Type: string
            - Name: instance_type
              Type: string
            - Name: total_neuron_core_utilization_hours
              Type: double
            - Name: allocated_neuron_core_utilization_hours
              Type: double
            - Name: borrowed_neuron_core_utilization_hours
              Type: double
            - Name: total_gpu_utilization_hours
              Type: double
            - Name: allocated_gpu_utilization_hours
              Type: double
            - Name: borrowed_gpu_utilization_hours
              Type: double
            - Name: total_vcpu_utilization_hours
              Type: double
            - Name: allocated_vcpu_utilization_hours
              Type: double
            - Name: borrowed_vcpu_utilization_hours
              Type: double
        PartitionKeys:
            - Name: year
              Type: string
            - Name: month
              Type: string
            - Name: day
              Type: string
            - Name: cluster
              Type: string
  HeartdubTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref UsageReportDatabase
      TableInput:
        Name: 'heartdub'
        Description: 'Image heartdub table for tracking connections'
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "parquet",
          "parquet.compression": "SNAPPY"
        }
        StorageDescriptor:
          Location: !Sub 's3://${UsageReportBucket}/raw/heartdub/'
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Columns:
            - Name: timestamp
              Type: timestamp
            - Name: cluster
              Type: string
        PartitionKeys:
          - Name: year
            Type: string
          - Name: month
            Type: string
          - Name: day
            Type: string
          - Name: hour
            Type: string
  AggregationRule:
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: "cron(0 0 * * ? *)" 
      State: ENABLED
      Targets:
        - Arn: !GetAtt AggregationLambda.Arn
          Id: "AthenaAggregationTarget"
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
        - !Ref LambdaExecutionRolePolicy
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt AggregationLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt AggregationRule.Arn
  LambdaExecutionRolePolicy:
    Type: "AWS::IAM::ManagedPolicy"
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action:
          - "s3:GetObject"
          - "s3:PutObject"
          Resource:
          - !Sub 'arn:aws:s3:::${UsageReportBucket}/*'
        - Effect: Allow
          Action:
            - athena:StartQueryExecution
            - athena:GetQueryExecution
          Resource:
          - '*'
  AggregationLambda:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt LambdaExecutionRole.Arn
      Handler: index.aggregate_all_data
      Runtime: python3.9
      Timeout: 900
      Code: 
        ZipFile: !Sub |
            import boto3
            import time
            from datetime import datetime, timedelta
            from enum import Enum

            class QueryType(Enum):
                CRESCENDO_SUMMARY = "crescendo_summary"
                CRESCENDO_DETAILED = "crescendo_detailed"
                NON_CRESCENDO_SUMMARY = "non_crescendo_summary"
                NON_CRESCENDO_DETAILED = "non_crescendo_detailed"

            class QueryTemplates:
                CRESCENDO_SUMMARY = """
                  INSERT into "${UsageReportDatabase}".summary_report
                  WITH latest_version_resource_states AS (
                    SELECT * FROM (
                      SELECT *, ROW_NUMBER() OVER (
                          PARTITION BY instance_type, timestamp, namespace, team
                          ORDER BY CAST(resource_version AS bigint) DESC
                        ) AS row_num
                      FROM "${UsageReportDatabase}".clusterqueue
                    ) ranked
                    WHERE row_num = 1 AND year = '{year}' AND month = '{month}' AND day = '{day}'
                  ),
                  resource_periods AS (
                      SELECT DISTINCT
                          CASE
                              WHEN ROW_NUMBER() OVER (PARTITION BY namespace, team, instance_type, year, month, day ORDER BY timestamp) = 1
                                  AND (gpu_total > 0 OR cpu_total > 0 OR neuron_core_total > 0)
                              THEN DATE_TRUNC('DAY', timestamp)
                              ELSE timestamp
                          END as period_start,
                          CASE
                              WHEN LEAD(timestamp) OVER(
                                  PARTITION BY namespace, team, instance_type, year, month, day
                                  ORDER BY timestamp
                              ) IS NULL 
                              THEN DATE_ADD('DAY', 1, DATE_TRUNC('DAY', timestamp))
                              ELSE LEAD(timestamp) OVER(
                                  PARTITION BY namespace, team, instance_type, year, month, day
                                  ORDER BY timestamp
                              )
                          END as period_end, namespace, team, instance_type, gpu_total, gpu_borrowed,
                          cpu_total, cpu_borrowed, neuron_core_total, neuron_core_borrowed, cluster
                      FROM latest_version_resource_states
                  ),
                  consolidated_periods AS (
                      SELECT 
                          DATE(period_start) as report_date, namespace, team, instance_type, gpu_total, gpu_borrowed,
                          cpu_total, cpu_borrowed, neuron_core_total, neuron_core_borrowed, period_start, period_end,
                          -- Calculate utilization hours for each resource type
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * gpu_total as total_gpu_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * (gpu_total - gpu_borrowed) as allocated_gpu_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * gpu_borrowed as borrowed_gpu_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * cpu_total as total_cpu_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * (cpu_total - cpu_borrowed) as allocated_cpu_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * cpu_borrowed as borrowed_cpu_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * neuron_core_total as total_neuron_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * (neuron_core_total - neuron_core_borrowed) as allocated_neuron_hours,
                          (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * neuron_core_borrowed as borrowed_neuron_hours,
                          cluster
                      FROM resource_periods
                      WHERE period_end IS NOT NULL
                  ),
                  daily_summary AS (
                      SELECT report_date, namespace, team, instance_type,
                          SUM(total_neuron_hours) as total_neuron_core_utilization_hours,
                          SUM(allocated_neuron_hours) as allocated_neuron_core_utilization_hours,
                          SUM(borrowed_neuron_hours) as borrowed_neuron_core_utilization_hours,
                          SUM(total_gpu_hours) as total_gpu_utilization_hours,
                          SUM(allocated_gpu_hours) as allocated_gpu_utilization_hours,
                          SUM(borrowed_gpu_hours) as borrowed_gpu_utilization_hours,
                          SUM(total_cpu_hours) as total_vcpu_utilization_hours,
                          SUM(allocated_cpu_hours) as allocated_vcpu_utilization_hours,
                          SUM(borrowed_cpu_hours) as borrowed_vcpu_utilization_hours,
                          CAST(YEAR(report_date) AS VARCHAR) as year,
                          LPAD(CAST(MONTH(report_date) AS VARCHAR), 2, '0') as month,
                          LPAD(CAST(DAY(report_date) AS VARCHAR), 2, '0') as day,
                          cluster
                      FROM consolidated_periods
                      GROUP BY 
                          report_date, namespace, team, instance_type, cluster
                  )
                  SELECT
                      report_date, namespace, team, instance_type,
                      total_neuron_core_utilization_hours,
                      allocated_neuron_core_utilization_hours,
                      borrowed_neuron_core_utilization_hours,
                      total_gpu_utilization_hours,
                      allocated_gpu_utilization_hours,
                      borrowed_gpu_utilization_hours,
                      total_vcpu_utilization_hours,
                      allocated_vcpu_utilization_hours,
                      borrowed_vcpu_utilization_hours,
                      year, month, day, cluster
                  FROM daily_summary;
                """

                CRESCENDO_DETAILED = """
                    Insert into "${UsageReportDatabase}".detailed_report
                    WITH latest_version_workload AS (
                      SELECT * FROM (
                        SELECT *, ROW_NUMBER() OVER (
                            PARTITION BY workload_name, timestamp, namespace, team, instance_type
                            ORDER BY CAST(resource_version AS bigint) DESC
                          ) AS row_num
                        FROM "${UsageReportDatabase}".workload
                      ) ranked
                      WHERE row_num = 1
                    ),
                    workload_state_changes AS (
                        SELECT DISTINCT
                            workload_name, timestamp, namespace, team, status, instance_type, task_priority_class,
                            CPU, GPU, neuron_core, admitted, finished, year, month, day, cluster
                        FROM latest_version_workload
                        WHERE year = '{year}' AND month = '{month}' AND day = '{day}'
                    ),

                    running_periods AS (
                        SELECT 
                            workload_name, timestamp as period_start,
                            CASE 
                              WHEN LEAD(timestamp) OVER(
                                  PARTITION BY workload_name, instance_type, year, month, day
                                  ORDER BY timestamp
                              ) is NULL and status = 'QuotaReserved-True' 
                              THEN DATE_ADD('DAY', 1, DATE_TRUNC('DAY', timestamp))
                              ELSE COALESCE(LEAD(timestamp) OVER(
                                      PARTITION BY workload_name, instance_type, year, month, day
                                      ORDER BY timestamp
                                  ), timestamp) 
                            END as period_end,
                            namespace, status,
                            CASE 
                              WHEN LEAD(status) OVER(
                                  PARTITION BY workload_name, instance_type, year, month, day
                                  ORDER BY timestamp
                              ) is NULL and status = 'QuotaReserved-True'
                              THEN 'QuotaReserved-True'
                              ELSE LEAD(status) OVER(
                                  PARTITION BY workload_name, instance_type, year, month, day
                                  ORDER BY timestamp
                              )
                            END as next_status,
                            team, instance_type, task_priority_class, CPU, GPU, neuron_core,
                            admitted, finished, year, month, day, cluster
                        FROM workload_state_changes
                    ),

                    filtered_running_periods AS (
                        SELECT *, LAG(period_end) OVER (PARTITION BY workload_name, instance_type ORDER BY period_start) AS prev_period_end
                        FROM running_periods
                        WHERE admitted = true
                    ),

                    running_grouped AS (
                        SELECT *,
                            CASE 
                                WHEN prev_period_end IS NULL THEN 1
                                WHEN date_diff('second', prev_period_end, period_start) > 0 THEN 1
                                ELSE 0
                            END AS is_new_group
                        FROM filtered_running_periods
                    ),

                    grouped_with_ids AS (
                        SELECT *, SUM(is_new_group) OVER (PARTITION BY workload_name, instance_type ORDER BY period_start ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS group_id
                        FROM running_grouped
                    ),

                    continuous_running_periods AS (
                        SELECT
                            workload_name, group_id, MIN(period_start) AS period_start, MAX(period_end) AS period_end, namespace,
                            CASE 
                                WHEN bool_or(finished) THEN 'Finished'
                                WHEN bool_or(next_status = 'Failed') OR bool_or(next_status IS NULL) THEN 'Failed'
                                WHEN bool_or(status = 'QuotaReserved-True' AND next_status = 'Evicted-True') THEN 'Preempted'
                                ELSE 'Running'
                            END AS status, team, instance_type, task_priority_class, CPU, GPU, neuron_core, year, month, day, cluster
                        FROM grouped_with_ids
                        GROUP BY 
                          workload_name, group_id, namespace, team, instance_type, task_priority_class, CPU, GPU, neuron_core, 
                          year, month, day, cluster
                    ),
                    team_grouped_utilization AS (
                        SELECT
                            workload_name, group_id, namespace, team, instance_type, task_priority_class,
                            MIN(period_start) AS period_start, MAX(period_end) AS period_end, MAX(status) AS status,
                            SUM(date_diff('second', period_start, period_end) / 3600.0 * CPU) AS cpu_utilization_hours,
                            SUM(date_diff('second', period_start, period_end) / 3600.0 * GPU) AS gpu_utilization_hours,
                            SUM(date_diff('second', period_start, period_end) / 3600.0 * neuron_core) AS neuron_utilization_hours,
                            COUNT(DISTINCT workload_name) AS workload_count,
                            SUM(CPU) AS cpu_count, SUM(GPU) AS gpu_count, SUM(neuron_core) AS neuron_count,
                            year, month, day, cluster
                        FROM continuous_running_periods
                        GROUP BY 
                            workload_name, group_id, namespace, team, instance_type, task_priority_class,
                            year, month, day, cluster
                    )
                    SELECT DATE(period_start) as report_date, period_start, period_end, namespace, team, workload_name as task_name,
                        instance_type as instance, workload_count as instance_count, status, neuron_utilization_hours as utilized_neuron_core_hours, 
                        CAST(neuron_count as double) as utilized_neuron_core_count, gpu_utilization_hours as utilized_gpu_hours, 
                        CAST(gpu_count as double) as utilized_gpu_count, cpu_utilization_hours as utilized_vcpu_hours, 
                        CAST(cpu_count as double) as utilized_vcpu_count, task_priority_class as priority_class, 
                        year, month, day, cluster
                    FROM team_grouped_utilization
                    ORDER BY report_date, task_name, instance;
                """

                NON_CRESCENDO_SUMMARY = """
                INSERT INTO "${UsageReportDatabase}".summary_report
                WITH latest_pod_state_changes AS (
                  SELECT *
                  FROM (
                    SELECT
                      *,
                      ROW_NUMBER() OVER (
                        PARTITION BY pod, timestamp, namespace, owner, instance_type
                        ORDER BY CAST(resource_version AS bigint) DESC
                      ) AS row_num
                    FROM "${UsageReportDatabase}".pod
                  ) ranked
                  WHERE row_num = 1
                ),
                pod_state_changes AS (
                    SELECT DISTINCT
                        pod, timestamp, namespace, status, owner, instance_type, job_type, CPU, GPU,
                        neuron_core, year, month, day, cluster
                    FROM latest_pod_state_changes
                    WHERE year = '{year}' AND month = '{month}' AND day = '{day}'
                    AND status != 'Unknown'
                ),
                running_periods AS (
                    SELECT 
                        pod,
                        CASE
                            WHEN ROW_NUMBER() OVER (PARTITION BY pod, year, month, day ORDER BY timestamp) = 1
                                AND status = 'Running'
                            THEN DATE_TRUNC('DAY', timestamp)
                            ELSE timestamp
                        END as period_start,
                        CASE
                            WHEN LEAD(timestamp) OVER(
                                PARTITION BY pod, instance_type, year, month, day
                                ORDER BY timestamp
                            ) IS NULL 
                            AND status = 'Running'
                            THEN DATE_ADD('DAY', 1, DATE_TRUNC('DAY', timestamp))
                            ELSE LEAD(timestamp) OVER(
                                PARTITION BY pod, instance_type, year, month, day
                                ORDER BY timestamp
                            )
                        END as period_end,
                        status, instance_type, namespace, CPU, GPU, neuron_core, owner, year, month, day, cluster
                    FROM pod_state_changes
                ),
                utilization_calc AS (
                    SELECT
                        DATE(period_start) as report_date,
                        namespace,
                        '' as team,
                        instance_type,
                        SUM(
                            CASE 
                                WHEN status = 'Running' AND period_end IS NOT NULL 
                                THEN (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * neuron_core
                                ELSE 0 
                            END
                        ) as total_neuron_core_utilization_hours,
                        0.0 as allocated_neuron_core_utilization_hours,
                        0.0 as borrowed_neuron_core_utilization_hours,
                        SUM(
                            CASE 
                                WHEN status = 'Running' AND period_end IS NOT NULL 
                                THEN (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * GPU
                                ELSE 0 
                            END
                        ) as total_gpu_utilization_hours,
                        0.0 as allocated_gpu_utilization_hours,
                        0.0 as borrowed_gpu_utilization_hours,
                        SUM(
                            CASE 
                                WHEN status = 'Running' AND period_end IS NOT NULL 
                                THEN (CAST(DATE_DIFF('second', period_start, period_end) AS DOUBLE) / 3600) * CPU
                                ELSE 0 
                            END
                        ) as total_vcpu_utilization_hours,
                        0.0 as allocated_vcpu_utilization_hours,
                        0.0 as borrowed_vcpu_utilization_hours,
                        CAST(YEAR(DATE(period_start)) AS VARCHAR) as year,
                        LPAD(CAST(MONTH(DATE(period_start)) AS VARCHAR), 2, '0') as month,
                        LPAD(CAST(DAY(DATE(period_start)) AS VARCHAR), 2, '0') as day,
                        cluster
                    FROM running_periods
                    GROUP BY 
                        DATE(period_start),
                        namespace, instance_type, owner, year, month, day, cluster
                )
                SELECT
                    report_date,
                    namespace,
                    team,
                    instance_type,
                    total_neuron_core_utilization_hours,
                    allocated_neuron_core_utilization_hours,
                    borrowed_neuron_core_utilization_hours,
                    total_gpu_utilization_hours,
                    allocated_gpu_utilization_hours,
                    borrowed_gpu_utilization_hours,
                    total_vcpu_utilization_hours,
                    allocated_vcpu_utilization_hours,
                    borrowed_vcpu_utilization_hours,
                    year,
                    month,
                    day,
                    cluster
                FROM utilization_calc;
                """

                NON_CRESCENDO_DETAILED = """
                    INSERT INTO "${UsageReportDatabase}".detailed_report
                    WITH latest_pod_state_changes AS (
                      SELECT *
                      FROM (
                        SELECT
                          *,
                          ROW_NUMBER() OVER (
                            PARTITION BY pod, timestamp, namespace, owner, instance_type
                            ORDER BY CAST(resource_version AS bigint) DESC
                          ) AS row_num
                        FROM "${UsageReportDatabase}".pod
                      ) ranked
                      WHERE row_num = 1
                    ),
                    pod_state_changes AS (
                        SELECT DISTINCT
                            namespace, pod, timestamp, status, owner as task_name, instance_type, job_type,
                            assigned_node, CPU, GPU, neuron_core, priority_class, year, month, day, cluster
                        FROM latest_pod_state_changes
                        WHERE year = '{year}' AND month = '{month}' AND day = '{day}'
                    ),
                    running_periods AS (
                        SELECT 
                            namespace,
                            pod,
                            timestamp as period_start,
                            CASE 
                                WHEN LEAD(timestamp) OVER(
                                    PARTITION BY pod, instance_type, year, month, day
                                    ORDER BY timestamp
                                ) is NULL and status = 'Running' 
                                THEN DATE_ADD('DAY', 1, DATE_TRUNC('DAY', timestamp))
                                ELSE COALESCE(
                                    LEAD(timestamp) OVER(
                                        PARTITION BY pod, instance_type, year, month, day
                                        ORDER BY timestamp
                                    ), 
                                    timestamp
                                ) 
                              END as period_end,
                            status,
                            CASE 
                                WHEN LEAD(status) OVER(
                                    PARTITION BY pod, instance_type, year, month, day
                                    ORDER BY timestamp
                                ) is NULL and status = 'Running'
                                THEN 'Running'
                                ELSE LEAD(status) OVER(
                                    PARTITION BY pod, instance_type, year, month, day
                                    ORDER BY timestamp
                                )
                            END as next_status,
                            task_name, instance_type, job_type, CPU, GPU, neuron_core, priority_class,
                            year, month, day, cluster
                        FROM pod_state_changes
                    ), 
                    filtered_running_periods AS (
                        SELECT 
                            *,
                            LAG(period_end) OVER (PARTITION BY pod ORDER BY period_start) AS prev_period_end
                        FROM running_periods
                        WHERE status = 'Running'
                    ),

                    running_grouped AS (
                        SELECT *,
                            CASE 
                                WHEN prev_period_end IS NULL THEN 1
                                WHEN date_diff('second', prev_period_end, period_start) > 0 THEN 1
                                ELSE 0
                            END AS is_new_group
                        FROM filtered_running_periods
                    ),

                    grouped_with_ids AS (
                        SELECT *,
                            SUM(is_new_group) OVER (PARTITION BY pod ORDER BY period_start ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS group_id
                        FROM running_grouped
                    ),

                    continuous_running_periods AS (
                        SELECT
                            namespace,
                            pod,
                            group_id,
                            MIN(period_start) AS period_start,
                            MAX(period_end) AS period_end,
                            CASE 
                                WHEN COUNT_IF(next_status = 'Succeeded') > 0 THEN 'Succeeded'
                                WHEN COUNT_IF(next_status = 'Failed') > 0 or COUNT_IF(next_status is NULL) > 0 THEN 'Failed'
                                WHEN bool_or(status = 'QuotaReserved-True' AND next_status = 'Evicted-True') THEN 'Preempted'
                                ELSE 'Running'
                            END AS status,
                            task_name,
                            instance_type,
                            job_type,
                            CPU,
                            GPU,
                            neuron_core,
                            priority_class,
                            year,
                            month,
                            day,
                            cluster
                        FROM grouped_with_ids
                        GROUP BY
                            namespace,
                            pod,
                            group_id,
                            task_name,
                            instance_type,
                            job_type,
                            CPU,
                            GPU,
                            neuron_core,
                            priority_class,
                            year,
                            month,
                            day,
                            cluster
                    ),
                    team_grouped_utilization AS (
                        SELECT
                            pod,
                            namespace,
                            task_name,
                            group_id,
                            instance_type,
                            job_type,
                            MIN(period_start) AS period_start,
                            MAX(period_end) AS period_end,
                            MAX(status) AS status,
                            SUM(date_diff('second', period_start, period_end) / 3600.0 * CPU) AS cpu_utilization_hours,
                            SUM(date_diff('second', period_start, period_end) / 3600.0 * GPU) AS gpu_utilization_hours,
                            SUM(date_diff('second', period_start, period_end) / 3600.0 * neuron_core) AS neuron_utilization_hours,
                            COUNT(DISTINCT pod) AS pod_count,
                            MAX(CPU) AS cpu_count,
                            MAX(GPU) AS gpu_count,
                            MAX(neuron_core) AS neuron_count,
                            priority_class,
                            year,
                            month,
                            day,
                            cluster
                        FROM continuous_running_periods
                        GROUP BY 
                            pod, namespace, task_name, group_id, instance_type, job_type,
                            priority_class, year, month, day, cluster
                    )
                    SELECT
                        DATE(period_start) as report_date, period_start, period_end, namespace, '' as team,
                        task_name, instance_type as instance, pod_count as instance_count, status,
                        neuron_utilization_hours as utilized_neuron_core_hours,
                        CAST(neuron_count as double) as utilized_neuron_core_count,
                        gpu_utilization_hours as utilized_gpu_hours,
                        CAST(gpu_count as double) as utilized_gpu_count,
                        cpu_utilization_hours as utilized_vcpu_hours,
                        CAST(cpu_count as double) as utilized_vcpu_count,
                        priority_class, year, month, day, cluster
                    FROM team_grouped_utilization
                    ORDER BY report_date, task_name, instance;
                """

            class AthenaQueryExecutor:
                def __init__(self, dry_run):
                    self.athena = boto3.client('athena')
                    self.output_location = "s3://${UsageReportBucket}/athena-results"
                    self.yesterday = (datetime.now() - timedelta(days=1)) if not dry_run else datetime.now()
                    self.query_mapping = {
                        QueryType.CRESCENDO_SUMMARY: QueryTemplates.CRESCENDO_SUMMARY,
                        QueryType.CRESCENDO_DETAILED: QueryTemplates.CRESCENDO_DETAILED,
                        QueryType.NON_CRESCENDO_SUMMARY: QueryTemplates.NON_CRESCENDO_SUMMARY,
                        QueryType.NON_CRESCENDO_DETAILED: QueryTemplates.NON_CRESCENDO_DETAILED
                    }

                def wait_for_query_completion(self, query_execution_id):
                    while True:
                        query_status = self.athena.get_query_execution(QueryExecutionId=query_execution_id)
                        status = query_status['QueryExecution']['Status']['State']
                        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                            return status, query_status
                        time.sleep(5)

                def execute_query(self, query):
                    response = self.athena.start_query_execution(
                        QueryString=query,
                        QueryExecutionContext={'Database': '${UsageReportDatabase}'},
                        ResultConfiguration={'OutputLocation': self.output_location}
                    )
                    status, query_status = self.wait_for_query_completion(response['QueryExecutionId'])
                    return status, query_status, response['QueryExecutionId']

                def format_response(self, query_type, status, query_status, query_execution_id):
                    if status == 'SUCCEEDED':
                        return {
                            'queryType': query_type.value,
                            'status': 'SUCCESS',
                            'message': f"Data aggregation completed for {self.yesterday}",
                            'queryExecutionId': query_execution_id
                        }
                    else:
                        error_message = query_status['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                        print(f"Query failed for {query_type.value}: {error_message}")
                        return {
                            'queryType': query_type.value,
                            'status': 'FAILED',
                            'message': f"Data aggregation failed: {error_message}",
                            'queryExecutionId': query_execution_id
                        }

                def get_query_template(self, query_type: QueryType) -> str:
                    return self.query_mapping.get(query_type, "")

                def prepare_query(self, query_template: str) -> str:
                    return query_template.replace('{year}', self.yesterday.strftime('%Y'))\
                                      .replace('{month}', self.yesterday.strftime('%m'))\
                                      .replace('{day}', self.yesterday.strftime('%d'))
                
                def repair_tables(self) -> bool:
                    tables = ['clusterqueue', 'workload', 'pod', 'heartdub']
                    for table in tables:
                        repair_query = f"MSCK REPAIR TABLE {table}"
                        status, query_status, query_execution_id = self.execute_query(repair_query)      
                        if status != "SUCCEEDED":
                            return False
                    return True
                        
                def execute_aggregation(self, query_type: QueryType):
                    try:
                        query_template = self.get_query_template(query_type)
                        if not query_template:
                            return {
                                'queryType': query_type.value,
                                'status': 'ERROR',
                                'message': f"No query template found for {query_type.value}",
                                'queryExecutionId': None
                            }
                        query = self.prepare_query(query_template)
                        status, query_status, query_execution_id = self.execute_query(query)
                        return self.format_response(query_type, status, query_status, query_execution_id)
                    except Exception as e:
                        return {
                            'queryType': query_type.value,
                            'status': 'ERROR',
                            'message': f"Execution error: {str(e)}",
                            'queryExecutionId': None
                        }

            def aggregate_all_data(event, context):
                """
                Main handler that executes all aggregations in parallel
                """
                dry_run = event.get('dry_run', False)
                executor = AthenaQueryExecutor(dry_run)
                if not executor.repair_tables():
                    return {
                          'statusCode': 500,
                          'body': f"Failed to repair database tables for querying."
                      }
                results = []
                for query_type in QueryType:
                    result = executor.execute_aggregation(query_type)
                    if result['status'] == 'SUCCESS':
                        results.append(result)
                    else:
                        return {
                            'statusCode': 500,
                            'body': f"Report aggregation failed: {result['message']}"
                        }
                return {
                    'statusCode': 200,
                    'body': f"Report aggregation completed for ",
                    'queryExecutionId': results
                }

Outputs:
  DatabaseName:
    Description: 'Name of the created database'
    Value: !Ref UsageReportDatabase
  
  UsageReportBucket:
    Description: 'Name of the created S3 Bucket'
    Value: !Ref UsageReportBucket